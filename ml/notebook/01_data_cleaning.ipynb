{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01 - Data Cleaning: Job Datasets\n",
    "\n",
    "This notebook loads and cleans all raw job datasets for the ArahBelajarAI skill demand forecasting pipeline.\n",
    "\n",
    "**Datasets:**\n",
    "1. `future_jobs_dataset.csv` (10,000 rows) - Emerging tech job postings\n",
    "2. `jobs.csv` (779 rows) - India-focused job postings\n",
    "3. `job_market.csv` (250 rows) - US/International tech jobs\n",
    "4. `jobstreet_all_job_dataset.csv` (2M+ rows) - Malaysia/Singapore jobs\n",
    "5. LinkedIn archive data - skill taxonomy & job-skill mappings\n",
    "\n",
    "**Output:** Cleaned parquet files in `ml/data/process/`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Setup & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /Users/kharisma.wardhana/Documents/project/python/DB10-Capstone-ArahBelajarAI/ml/.venv/lib/python3.10/site-packages (2.3.3)\n",
      "Requirement already satisfied: numpy in /Users/kharisma.wardhana/Documents/project/python/DB10-Capstone-ArahBelajarAI/ml/.venv/lib/python3.10/site-packages (2.2.6)\n",
      "Requirement already satisfied: pyarrow in /Users/kharisma.wardhana/Documents/project/python/DB10-Capstone-ArahBelajarAI/ml/.venv/lib/python3.10/site-packages (23.0.0)\n",
      "Requirement already satisfied: fastparquet in /Users/kharisma.wardhana/Documents/project/python/DB10-Capstone-ArahBelajarAI/ml/.venv/lib/python3.10/site-packages (2025.12.0)\n",
      "Collecting tqdm\n",
      "  Downloading tqdm-4.67.3-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/kharisma.wardhana/Documents/project/python/DB10-Capstone-ArahBelajarAI/ml/.venv/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/kharisma.wardhana/Documents/project/python/DB10-Capstone-ArahBelajarAI/ml/.venv/lib/python3.10/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/kharisma.wardhana/Documents/project/python/DB10-Capstone-ArahBelajarAI/ml/.venv/lib/python3.10/site-packages (from pandas) (2025.3)\n",
      "Requirement already satisfied: cramjam>=2.3 in /Users/kharisma.wardhana/Documents/project/python/DB10-Capstone-ArahBelajarAI/ml/.venv/lib/python3.10/site-packages (from fastparquet) (2.11.0)\n",
      "Requirement already satisfied: fsspec in /Users/kharisma.wardhana/Documents/project/python/DB10-Capstone-ArahBelajarAI/ml/.venv/lib/python3.10/site-packages (from fastparquet) (2026.2.0)\n",
      "Requirement already satisfied: packaging in /Users/kharisma.wardhana/Documents/project/python/DB10-Capstone-ArahBelajarAI/ml/.venv/lib/python3.10/site-packages (from fastparquet) (26.0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/kharisma.wardhana/Documents/project/python/DB10-Capstone-ArahBelajarAI/ml/.venv/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Downloading tqdm-4.67.3-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm\n",
      "Successfully installed tqdm-4.67.3\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m26.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas numpy pyarrow fastparquet tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directories configured:\n",
      "  Raw data: /Users/kharisma.wardhana/Documents/project/python/DB10-Capstone-ArahBelajarAI/ml/data/raw\n",
      "  Job data: /Users/kharisma.wardhana/Documents/project/python/DB10-Capstone-ArahBelajarAI/ml/data/raw/job\n",
      "  LinkedIn: /Users/kharisma.wardhana/Documents/project/python/DB10-Capstone-ArahBelajarAI/ml/data/raw/linkedin_archive\n",
      "  Output:   /Users/kharisma.wardhana/Documents/project/python/DB10-Capstone-ArahBelajarAI/ml/data/process\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "RAW_DIR = Path('../data/raw')\n",
    "JOB_DIR = RAW_DIR / 'job'\n",
    "LINKEDIN_DIR = RAW_DIR / 'linkedin_archive'\n",
    "PROCESS_DIR = Path('../data/process')\n",
    "PROCESS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print('Directories configured:')\n",
    "print(f'  Raw data: {RAW_DIR.resolve()}')\n",
    "print(f'  Job data: {JOB_DIR.resolve()}')\n",
    "print(f'  LinkedIn: {LINKEDIN_DIR.resolve()}')\n",
    "print(f'  Output:   {PROCESS_DIR.resolve()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for data quality reporting\n",
    "def data_quality_report(df, name):\n",
    "    \"\"\"Print a concise data quality summary for a DataFrame.\"\"\"\n",
    "    print(f'\\n{\"=\" * 60}')\n",
    "    print(f'Dataset: {name}')\n",
    "    print(f'{\"=\" * 60}')\n",
    "    print(f'Shape: {df.shape[0]:,} rows x {df.shape[1]} columns')\n",
    "    print(f'\\nColumn types:')\n",
    "    for col in df.columns:\n",
    "        null_count = df[col].isnull().sum()\n",
    "        null_pct = null_count / len(df) * 100\n",
    "        print(f'  {col:30s} {str(df[col].dtype):15s} nulls: {null_count:6d} ({null_pct:.1f}%)')\n",
    "    \n",
    "    dupes = df.duplicated().sum()\n",
    "    print(f'\\nExact duplicates: {dupes}')\n",
    "    print(f'Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.1f} MB')\n",
    "    return {'name': name, 'rows': len(df), 'cols': len(df.columns), 'duplicates': dupes}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Clean `future_jobs_dataset.csv` (10K rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Dataset: future_jobs (raw)\n",
      "============================================================\n",
      "Shape: 10,000 rows x 9 columns\n",
      "\n",
      "Column types:\n",
      "  job_id                         int64           nulls:      0 (0.0%)\n",
      "  job_title                      object          nulls:      0 (0.0%)\n",
      "  industry                       object          nulls:      0 (0.0%)\n",
      "  location                       object          nulls:      0 (0.0%)\n",
      "  salary_usd                     int64           nulls:      0 (0.0%)\n",
      "  skills_required                object          nulls:      0 (0.0%)\n",
      "  remote_option                  object          nulls:      0 (0.0%)\n",
      "  company_size                   object          nulls:      0 (0.0%)\n",
      "  posting_date                   object          nulls:      0 (0.0%)\n",
      "\n",
      "Exact duplicates: 0\n",
      "Memory usage: 4.7 MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'name': 'future_jobs (raw)',\n",
       " 'rows': 10000,\n",
       " 'cols': 9,\n",
       " 'duplicates': np.int64(0)}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_future = pd.read_csv(JOB_DIR / 'future_jobs_dataset.csv')\n",
    "data_quality_report(df_future, 'future_jobs (raw)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Industries: {'Quantum Computing': 2519, 'Blockchain': 2499, 'AI': 2492, 'Green Tech': 2490}\n",
      "\n",
      "Locations: {'New York': 1689, 'Singapore': 1682, 'Tokyo': 1673, 'Dubai': 1660, 'London': 1656, 'Berlin': 1640}\n",
      "\n",
      "Remote options: {'Yes': 5089, 'No': 4911}\n",
      "\n",
      "Company sizes: {'Large': 3385, 'Medium': 3328, 'Small': 3287}\n",
      "\n",
      "Salary range: $50,013 - $249,990\n",
      "Date range: 2025-01-01 to 2025-12-31\n"
     ]
    }
   ],
   "source": [
    "# Inspect value distributions\n",
    "print('Industries:', df_future['industry'].value_counts().to_dict())\n",
    "print('\\nLocations:', df_future['location'].value_counts().to_dict())\n",
    "print('\\nRemote options:', df_future['remote_option'].value_counts().to_dict())\n",
    "print('\\nCompany sizes:', df_future['company_size'].value_counts().to_dict())\n",
    "print(f'\\nSalary range: ${df_future[\"salary_usd\"].min():,.0f} - ${df_future[\"salary_usd\"].max():,.0f}')\n",
    "print(f'Date range: {df_future[\"posting_date\"].min()} to {df_future[\"posting_date\"].max()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicates removed: 0\n",
      "Invalid salaries: 0\n",
      "\n",
      "============================================================\n",
      "Dataset: future_jobs (cleaned)\n",
      "============================================================\n",
      "Shape: 10,000 rows x 10 columns\n",
      "\n",
      "Column types:\n",
      "  job_id                         int64           nulls:      0 (0.0%)\n",
      "  job_title                      object          nulls:      0 (0.0%)\n",
      "  industry                       object          nulls:      0 (0.0%)\n",
      "  location                       object          nulls:      0 (0.0%)\n",
      "  salary_usd                     int64           nulls:      0 (0.0%)\n",
      "  skills_required                object          nulls:      0 (0.0%)\n",
      "  remote_option                  bool            nulls:      0 (0.0%)\n",
      "  company_size                   category        nulls:      0 (0.0%)\n",
      "  posting_date                   datetime64[ns]  nulls:      0 (0.0%)\n",
      "  source                         object          nulls:      0 (0.0%)\n",
      "\n",
      "Exact duplicates: 0\n",
      "Memory usage: 3.7 MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>job_id</th>\n",
       "      <th>job_title</th>\n",
       "      <th>industry</th>\n",
       "      <th>location</th>\n",
       "      <th>salary_usd</th>\n",
       "      <th>skills_required</th>\n",
       "      <th>remote_option</th>\n",
       "      <th>company_size</th>\n",
       "      <th>posting_date</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Quantum Researcher</td>\n",
       "      <td>Quantum Computing</td>\n",
       "      <td>Singapore</td>\n",
       "      <td>175780</td>\n",
       "      <td>Linear Algebra, Quantum Algorithms</td>\n",
       "      <td>False</td>\n",
       "      <td>Large</td>\n",
       "      <td>2025-07-22</td>\n",
       "      <td>future_jobs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Renewable Energy Engineer</td>\n",
       "      <td>Green Tech</td>\n",
       "      <td>Singapore</td>\n",
       "      <td>137481</td>\n",
       "      <td>Climate Data Analysis, Energy Modeling</td>\n",
       "      <td>True</td>\n",
       "      <td>Large</td>\n",
       "      <td>2025-09-26</td>\n",
       "      <td>future_jobs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Quantum Researcher</td>\n",
       "      <td>Quantum Computing</td>\n",
       "      <td>Tokyo</td>\n",
       "      <td>182081</td>\n",
       "      <td>Linear Algebra, Qiskit</td>\n",
       "      <td>False</td>\n",
       "      <td>Medium</td>\n",
       "      <td>2025-12-31</td>\n",
       "      <td>future_jobs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Sustainability Analyst</td>\n",
       "      <td>Green Tech</td>\n",
       "      <td>Singapore</td>\n",
       "      <td>113822</td>\n",
       "      <td>Climate Data Analysis, Energy Modeling</td>\n",
       "      <td>False</td>\n",
       "      <td>Large</td>\n",
       "      <td>2025-05-29</td>\n",
       "      <td>future_jobs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Smart Contract Engineer</td>\n",
       "      <td>Blockchain</td>\n",
       "      <td>London</td>\n",
       "      <td>92575</td>\n",
       "      <td>Rust, Solidity</td>\n",
       "      <td>True</td>\n",
       "      <td>Small</td>\n",
       "      <td>2025-03-30</td>\n",
       "      <td>future_jobs</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   job_id                  job_title           industry   location  \\\n",
       "0       1         Quantum Researcher  Quantum Computing  Singapore   \n",
       "1       2  Renewable Energy Engineer         Green Tech  Singapore   \n",
       "2       3         Quantum Researcher  Quantum Computing      Tokyo   \n",
       "3       4     Sustainability Analyst         Green Tech  Singapore   \n",
       "4       5    Smart Contract Engineer         Blockchain     London   \n",
       "\n",
       "   salary_usd                         skills_required  remote_option  \\\n",
       "0      175780      Linear Algebra, Quantum Algorithms          False   \n",
       "1      137481  Climate Data Analysis, Energy Modeling           True   \n",
       "2      182081                  Linear Algebra, Qiskit          False   \n",
       "3      113822  Climate Data Analysis, Energy Modeling          False   \n",
       "4       92575                          Rust, Solidity           True   \n",
       "\n",
       "  company_size posting_date       source  \n",
       "0        Large   2025-07-22  future_jobs  \n",
       "1        Large   2025-09-26  future_jobs  \n",
       "2       Medium   2025-12-31  future_jobs  \n",
       "3        Large   2025-05-29  future_jobs  \n",
       "4        Small   2025-03-30  future_jobs  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Clean future_jobs\n",
    "df_future_clean = df_future.copy()\n",
    "\n",
    "# Drop duplicates on job_id\n",
    "n_before = len(df_future_clean)\n",
    "df_future_clean = df_future_clean.drop_duplicates(subset=['job_id'])\n",
    "print(f'Duplicates removed: {n_before - len(df_future_clean)}')\n",
    "\n",
    "# Parse posting_date to datetime\n",
    "df_future_clean['posting_date'] = pd.to_datetime(df_future_clean['posting_date'])\n",
    "\n",
    "# Convert remote_option to boolean\n",
    "df_future_clean['remote_option'] = df_future_clean['remote_option'].map({'Yes': True, 'No': False})\n",
    "\n",
    "# Convert company_size to ordered categorical\n",
    "df_future_clean['company_size'] = pd.Categorical(\n",
    "    df_future_clean['company_size'],\n",
    "    categories=['Small', 'Medium', 'Large'],\n",
    "    ordered=True\n",
    ")\n",
    "\n",
    "# Validate salary_usd\n",
    "invalid_salary = df_future_clean['salary_usd'].isna() | (df_future_clean['salary_usd'] <= 0)\n",
    "print(f'Invalid salaries: {invalid_salary.sum()}')\n",
    "\n",
    "# Add source column\n",
    "df_future_clean['source'] = 'future_jobs'\n",
    "\n",
    "data_quality_report(df_future_clean, 'future_jobs (cleaned)')\n",
    "df_future_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skills per row: {2: 10000}\n",
      "\n",
      "Sample skills: ['Linear Algebra, Quantum Algorithms', 'Climate Data Analysis, Energy Modeling', 'Linear Algebra, Qiskit', 'Climate Data Analysis, Energy Modeling', 'Rust, Solidity', 'Solidity, Rust', 'Climate Data Analysis, Energy Modeling', 'Qiskit, Quantum Algorithms', 'Climate Data Analysis, Energy Modeling', 'Ethereum, Solidity']\n"
     ]
    }
   ],
   "source": [
    "# NOTE: This dataset appears SYNTHETIC\n",
    "# Evidence: near-uniform distribution across industries (~2500 each),\n",
    "# locations (~1660 each), and exactly 2 skills per row.\n",
    "# Temporal analysis from this dataset should be interpreted with caution.\n",
    "print('Skills per row:', df_future_clean['skills_required'].str.count(',').add(1).value_counts().to_dict())\n",
    "print('\\nSample skills:', df_future_clean['skills_required'].head(10).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: cleaned_future_jobs.parquet (10,000 rows)\n"
     ]
    }
   ],
   "source": [
    "df_future_clean.to_parquet(PROCESS_DIR / 'cleaned_future_jobs.parquet', index=False)\n",
    "print(f'Saved: cleaned_future_jobs.parquet ({len(df_future_clean):,} rows)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Clean `jobs.csv` (India, 779 rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Dataset: india_jobs (raw)\n",
      "============================================================\n",
      "Shape: 780 rows x 8 columns\n",
      "\n",
      "Column types:\n",
      "  title                          object          nulls:      0 (0.0%)\n",
      "  company                        object          nulls:      0 (0.0%)\n",
      "  ratings                        float64         nulls:     34 (4.4%)\n",
      "  reviews                        object          nulls:     34 (4.4%)\n",
      "  experience                     object          nulls:      0 (0.0%)\n",
      "  city                           object          nulls:      0 (0.0%)\n",
      "  job_description                object          nulls:      0 (0.0%)\n",
      "  skills                         object          nulls:      0 (0.0%)\n",
      "\n",
      "Exact duplicates: 748\n",
      "Memory usage: 0.5 MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'name': 'india_jobs (raw)',\n",
       " 'rows': 780,\n",
       " 'cols': 8,\n",
       " 'duplicates': np.int64(748)}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_india = pd.read_csv(JOB_DIR / 'jobs.csv')\n",
    "data_quality_report(df_india, 'india_jobs (raw)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>company</th>\n",
       "      <th>ratings</th>\n",
       "      <th>reviews</th>\n",
       "      <th>experience</th>\n",
       "      <th>city</th>\n",
       "      <th>job_description</th>\n",
       "      <th>skills</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Associate Staff Engineer (Data Science)</td>\n",
       "      <td>Nagarro</td>\n",
       "      <td>3.9</td>\n",
       "      <td>4711  Reviews</td>\n",
       "      <td>5-7 Yrs</td>\n",
       "      <td>Hybrid - Bangalore Rural, India</td>\n",
       "      <td>Bachelor s or master s degree in computer scie...</td>\n",
       "      <td>NLP, Machine Learning on Azure, Data science o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Associate Staff Engineer (Data Science)</td>\n",
       "      <td>Nagarro</td>\n",
       "      <td>3.9</td>\n",
       "      <td>4711  Reviews</td>\n",
       "      <td>5-7 Yrs</td>\n",
       "      <td>Hybrid - Bangalore Rural, India</td>\n",
       "      <td>Bachelor s or master s degree in computer scie...</td>\n",
       "      <td>NLP, Machine Learning on Azure, Data science o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AI Engineer</td>\n",
       "      <td>Naukri</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6-10 Yrs</td>\n",
       "      <td>Mumbai, Pune</td>\n",
       "      <td>Preferred Qualifications (Good to Have). Exper...</td>\n",
       "      <td>Generative Ai, Machine Learning, Python, Natur...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     title  company  ratings        reviews  \\\n",
       "0  Associate Staff Engineer (Data Science)  Nagarro      3.9  4711  Reviews   \n",
       "1  Associate Staff Engineer (Data Science)  Nagarro      3.9  4711  Reviews   \n",
       "2                              AI Engineer   Naukri      NaN            NaN   \n",
       "\n",
       "  experience                             city  \\\n",
       "0    5-7 Yrs  Hybrid - Bangalore Rural, India   \n",
       "1    5-7 Yrs  Hybrid - Bangalore Rural, India   \n",
       "2   6-10 Yrs                     Mumbai, Pune   \n",
       "\n",
       "                                     job_description  \\\n",
       "0  Bachelor s or master s degree in computer scie...   \n",
       "1  Bachelor s or master s degree in computer scie...   \n",
       "2  Preferred Qualifications (Good to Have). Exper...   \n",
       "\n",
       "                                              skills  \n",
       "0  NLP, Machine Learning on Azure, Data science o...  \n",
       "1  NLP, Machine Learning on Azure, Data science o...  \n",
       "2  Generative Ai, Machine Learning, Python, Natur...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_india.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact duplicates removed: 748\n",
      "Ratings - valid: 31, null: 1\n",
      "\n",
      "============================================================\n",
      "Dataset: india_jobs (cleaned)\n",
      "============================================================\n",
      "Shape: 32 rows x 14 columns\n",
      "\n",
      "Column types:\n",
      "  title                          object          nulls:      0 (0.0%)\n",
      "  company                        object          nulls:      0 (0.0%)\n",
      "  ratings                        float64         nulls:      1 (3.1%)\n",
      "  reviews                        float64         nulls:      1 (3.1%)\n",
      "  experience                     object          nulls:      0 (0.0%)\n",
      "  city                           object          nulls:      0 (0.0%)\n",
      "  job_description                object          nulls:      0 (0.0%)\n",
      "  skills                         object          nulls:      0 (0.0%)\n",
      "  experience_min                 int64           nulls:      0 (0.0%)\n",
      "  experience_max                 int64           nulls:      0 (0.0%)\n",
      "  work_mode                      object          nulls:      0 (0.0%)\n",
      "  city_clean                     object          nulls:      0 (0.0%)\n",
      "  posting_date                   datetime64[ns]  nulls:     32 (100.0%)\n",
      "  source                         object          nulls:      0 (0.0%)\n",
      "\n",
      "Exact duplicates: 0\n",
      "Memory usage: 0.0 MB\n",
      "\n",
      "Work modes: {'Onsite': 24, 'Hybrid': 8}\n"
     ]
    }
   ],
   "source": [
    "df_india_clean = df_india.copy()\n",
    "\n",
    "# 1. Drop exact duplicates\n",
    "n_before = len(df_india_clean)\n",
    "df_india_clean = df_india_clean.drop_duplicates()\n",
    "print(f'Exact duplicates removed: {n_before - len(df_india_clean)}')\n",
    "\n",
    "# 2. Clean ratings: replace 'NA' string with NaN, convert to float\n",
    "df_india_clean['ratings'] = pd.to_numeric(df_india_clean['ratings'], errors='coerce')\n",
    "print(f'Ratings - valid: {df_india_clean[\"ratings\"].notna().sum()}, null: {df_india_clean[\"ratings\"].isna().sum()}')\n",
    "\n",
    "# 3. Clean reviews: strip 'Reviews' suffix, convert to numeric\n",
    "df_india_clean['reviews'] = (\n",
    "    df_india_clean['reviews']\n",
    "    .astype(str)\n",
    "    .str.replace(r'\\s*Reviews?', '', regex=True)\n",
    "    .str.strip()\n",
    ")\n",
    "df_india_clean['reviews'] = pd.to_numeric(df_india_clean['reviews'], errors='coerce')\n",
    "\n",
    "# 4. Parse experience: extract min/max years from strings like \"5-7 Yrs\"\n",
    "exp_pattern = r'(\\d+)-(\\d+)\\s*Yrs?'\n",
    "exp_extracted = df_india_clean['experience'].str.extract(exp_pattern)\n",
    "df_india_clean['experience_min'] = pd.to_numeric(exp_extracted[0], errors='coerce')\n",
    "df_india_clean['experience_max'] = pd.to_numeric(exp_extracted[1], errors='coerce')\n",
    "\n",
    "# 5. Clean city: split work mode and city name\n",
    "def parse_city(city_str):\n",
    "    if not isinstance(city_str, str):\n",
    "        return pd.Series({'work_mode': np.nan, 'city_clean': np.nan})\n",
    "    if 'Hybrid - ' in city_str:\n",
    "        return pd.Series({'work_mode': 'Hybrid', 'city_clean': city_str.replace('Hybrid - ', '')})\n",
    "    elif 'Remote' in city_str:\n",
    "        return pd.Series({'work_mode': 'Remote', 'city_clean': city_str.replace('Remote - ', '')})\n",
    "    else:\n",
    "        return pd.Series({'work_mode': 'Onsite', 'city_clean': city_str})\n",
    "\n",
    "city_parsed = df_india_clean['city'].apply(parse_city)\n",
    "df_india_clean['work_mode'] = city_parsed['work_mode']\n",
    "df_india_clean['city_clean'] = city_parsed['city_clean']\n",
    "\n",
    "# 6. No date column - add placeholder\n",
    "df_india_clean['posting_date'] = pd.NaT\n",
    "\n",
    "# 7. Add source\n",
    "df_india_clean['source'] = 'india_jobs'\n",
    "\n",
    "data_quality_report(df_india_clean, 'india_jobs (cleaned)')\n",
    "print(f'\\nWork modes: {df_india_clean[\"work_mode\"].value_counts().to_dict()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: cleaned_india_jobs.parquet (32 rows)\n"
     ]
    }
   ],
   "source": [
    "df_india_clean.to_parquet(PROCESS_DIR / 'cleaned_india_jobs.parquet', index=False)\n",
    "print(f'Saved: cleaned_india_jobs.parquet ({len(df_india_clean):,} rows)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Clean `job_market.csv` (250 rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Dataset: job_market (raw)\n",
      "============================================================\n",
      "Shape: 250 rows x 10 columns\n",
      "\n",
      "Column types:\n",
      "  job_title                      object          nulls:      0 (0.0%)\n",
      "  company                        object          nulls:      0 (0.0%)\n",
      "  location                       object          nulls:      0 (0.0%)\n",
      "  job_type                       object          nulls:     29 (11.6%)\n",
      "  category                       object          nulls:     20 (8.0%)\n",
      "  salary_min                     int64           nulls:      0 (0.0%)\n",
      "  salary_max                     int64           nulls:      0 (0.0%)\n",
      "  experience_required            float64         nulls:     43 (17.2%)\n",
      "  publication_date               object          nulls:      0 (0.0%)\n",
      "  skills                         object          nulls:     50 (20.0%)\n",
      "\n",
      "Exact duplicates: 0\n",
      "Memory usage: 0.1 MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'name': 'job_market (raw)',\n",
       " 'rows': 250,\n",
       " 'cols': 10,\n",
       " 'duplicates': np.int64(0)}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_market = pd.read_csv(JOB_DIR / 'job_market.csv')\n",
    "data_quality_report(df_market, 'job_market (raw)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>job_title</th>\n",
       "      <th>company</th>\n",
       "      <th>location</th>\n",
       "      <th>job_type</th>\n",
       "      <th>category</th>\n",
       "      <th>salary_min</th>\n",
       "      <th>salary_max</th>\n",
       "      <th>experience_required</th>\n",
       "      <th>publication_date</th>\n",
       "      <th>skills</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Engineering Manager</td>\n",
       "      <td>DataInc</td>\n",
       "      <td>San Francisco, CA</td>\n",
       "      <td>Remote</td>\n",
       "      <td>Technology</td>\n",
       "      <td>151082</td>\n",
       "      <td>291345</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2025-11-27</td>\n",
       "      <td>AWS, Agile, Machine Learning, Kubernetes, Mong...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Engineering Manager</td>\n",
       "      <td>EnterpriseHub</td>\n",
       "      <td>New York, NY</td>\n",
       "      <td>Remote</td>\n",
       "      <td>Technology</td>\n",
       "      <td>156891</td>\n",
       "      <td>280075</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2025-11-27</td>\n",
       "      <td>Java, Agile, Git, SQL, Ruby, Go</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Engineering Manager</td>\n",
       "      <td>StartupXYZ</td>\n",
       "      <td>Seattle, WA</td>\n",
       "      <td>Part-time</td>\n",
       "      <td>Technology</td>\n",
       "      <td>152134</td>\n",
       "      <td>280310</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2025-11-27</td>\n",
       "      <td>AWS, Python, Kubernetes, Git</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             job_title        company           location   job_type  \\\n",
       "0  Engineering Manager        DataInc  San Francisco, CA     Remote   \n",
       "1  Engineering Manager  EnterpriseHub       New York, NY     Remote   \n",
       "2  Engineering Manager     StartupXYZ        Seattle, WA  Part-time   \n",
       "\n",
       "     category  salary_min  salary_max  experience_required publication_date  \\\n",
       "0  Technology      151082      291345                  4.0       2025-11-27   \n",
       "1  Technology      156891      280075                  3.0       2025-11-27   \n",
       "2  Technology      152134      280310                  4.0       2025-11-27   \n",
       "\n",
       "                                              skills  \n",
       "0  AWS, Agile, Machine Learning, Kubernetes, Mong...  \n",
       "1                    Java, Agile, Git, SQL, Ruby, Go  \n",
       "2                       AWS, Python, Kubernetes, Git  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_market.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicates removed: 0\n",
      "Date range: 2025-11-27 00:00:00 to 2025-11-27 00:00:00\n",
      "Rows where salary_min > salary_max: 0\n",
      "Job types: {'Remote': 55, 'Full-time': 50, 'Contract': 50, 'Part-time': 49, 'berufserfahren': 6, 'Full time': 3, 'Working student': 3, 'professional / experienced': 2, 'manager': 1, 'Internship': 1, 'berufseinstieg': 1}\n",
      "\n",
      "============================================================\n",
      "Dataset: job_market (cleaned)\n",
      "============================================================\n",
      "Shape: 250 rows x 11 columns\n",
      "\n",
      "Column types:\n",
      "  job_title                      object          nulls:      0 (0.0%)\n",
      "  company                        object          nulls:      0 (0.0%)\n",
      "  location                       object          nulls:      0 (0.0%)\n",
      "  job_type                       object          nulls:     29 (11.6%)\n",
      "  category                       object          nulls:     20 (8.0%)\n",
      "  salary_min                     int64           nulls:      0 (0.0%)\n",
      "  salary_max                     int64           nulls:      0 (0.0%)\n",
      "  experience_required            Int64           nulls:     43 (17.2%)\n",
      "  publication_date               datetime64[ns]  nulls:     50 (20.0%)\n",
      "  skills                         object          nulls:     50 (20.0%)\n",
      "  source                         object          nulls:      0 (0.0%)\n",
      "\n",
      "Exact duplicates: 3\n",
      "Memory usage: 0.1 MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'name': 'job_market (cleaned)',\n",
       " 'rows': 250,\n",
       " 'cols': 11,\n",
       " 'duplicates': np.int64(3)}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_market_clean = df_market.copy()\n",
    "\n",
    "# 1. Drop duplicates\n",
    "n_before = len(df_market_clean)\n",
    "df_market_clean = df_market_clean.drop_duplicates()\n",
    "print(f'Duplicates removed: {n_before - len(df_market_clean)}')\n",
    "\n",
    "# 2. Parse publication_date to datetime\n",
    "df_market_clean['publication_date'] = pd.to_datetime(df_market_clean['publication_date'], errors='coerce')\n",
    "print(f'Date range: {df_market_clean[\"publication_date\"].min()} to {df_market_clean[\"publication_date\"].max()}')\n",
    "# NOTE: All rows have the same date (2025-11-27) - single snapshot, no temporal analysis possible\n",
    "\n",
    "# 3. Validate salaries\n",
    "invalid_salary = df_market_clean['salary_min'] > df_market_clean['salary_max']\n",
    "print(f'Rows where salary_min > salary_max: {invalid_salary.sum()}')\n",
    "if invalid_salary.any():\n",
    "    # Swap if reversed\n",
    "    mask = invalid_salary\n",
    "    df_market_clean.loc[mask, ['salary_min', 'salary_max']] = (\n",
    "        df_market_clean.loc[mask, ['salary_max', 'salary_min']].values\n",
    "    )\n",
    "\n",
    "# 4. Parse experience_required as int\n",
    "df_market_clean['experience_required'] = pd.to_numeric(\n",
    "    df_market_clean['experience_required'], errors='coerce'\n",
    ").astype('Int64')\n",
    "\n",
    "# 5. Normalize job_type\n",
    "print(f'Job types: {df_market_clean[\"job_type\"].value_counts().to_dict()}')\n",
    "\n",
    "# 6. Add source\n",
    "df_market_clean['source'] = 'job_market'\n",
    "\n",
    "data_quality_report(df_market_clean, 'job_market (cleaned)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: cleaned_job_market.parquet (250 rows)\n"
     ]
    }
   ],
   "source": [
    "df_market_clean.to_parquet(PROCESS_DIR / 'cleaned_job_market.parquet', index=False)\n",
    "print(f'Saved: cleaned_job_market.parquet ({len(df_market_clean):,} rows)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Clean `jobstreet_all_job_dataset.csv` (2M+ rows, Memory-Critical)\n",
    "\n",
    "**Strategy:** Chunked loading with `engine='python'` to handle multiline descriptions, then filter to tech-related categories only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading jobstreet dataset in chunks...\n",
      "Filtering to categories: {'Information & Communication Technology', 'Science & Technology', 'Engineering'}\n",
      "\n",
      "Total rows read: 69,024\n",
      "Total tech rows kept: 18,204\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Tech-related categories to keep\n",
    "TECH_CATEGORIES = {\n",
    "    'Information & Communication Technology',\n",
    "    'Engineering',\n",
    "    'Science & Technology',\n",
    "}\n",
    "\n",
    "dtype_spec = {\n",
    "    'job_id': 'float64',\n",
    "    'job_title': 'str',\n",
    "    'company': 'str',\n",
    "    'descriptions': 'str',\n",
    "    'location': 'str',\n",
    "    'category': 'str',\n",
    "    'subcategory': 'str',\n",
    "    'role': 'str',\n",
    "    'type': 'str',\n",
    "    'salary': 'str',\n",
    "    'listingDate': 'str',\n",
    "}\n",
    "\n",
    "print('Loading jobstreet dataset in chunks...')\n",
    "print(f'Filtering to categories: {TECH_CATEGORIES}')\n",
    "\n",
    "cleaned_chunks = []\n",
    "total_raw = 0\n",
    "total_kept = 0\n",
    "\n",
    "try:\n",
    "    chunks = pd.read_csv(\n",
    "        JOB_DIR / 'jobstreet_all_job_dataset.csv',\n",
    "        dtype=dtype_spec,\n",
    "        chunksize=50_000,\n",
    "        on_bad_lines='skip',\n",
    "        engine='python',\n",
    "    )\n",
    "    \n",
    "    for i, chunk in enumerate(chunks):\n",
    "        total_raw += len(chunk)\n",
    "        \n",
    "        # Filter to tech categories\n",
    "        chunk = chunk[chunk['category'].isin(TECH_CATEGORIES)]\n",
    "        \n",
    "        if len(chunk) == 0:\n",
    "            continue\n",
    "        \n",
    "        # Drop rows with NaN job_id or empty job_title\n",
    "        chunk = chunk.dropna(subset=['job_id', 'job_title'])\n",
    "        \n",
    "        # Convert job_id to int\n",
    "        chunk['job_id'] = chunk['job_id'].astype(int)\n",
    "        \n",
    "        # Strip HTML tags from descriptions\n",
    "        chunk['descriptions'] = chunk['descriptions'].fillna('').apply(\n",
    "            lambda x: re.sub(r'<[^>]+>', ' ', str(x))\n",
    "        )\n",
    "        \n",
    "        # Truncate very long descriptions\n",
    "        chunk['descriptions'] = chunk['descriptions'].str[:5000]\n",
    "        \n",
    "        total_kept += len(chunk)\n",
    "        cleaned_chunks.append(chunk)\n",
    "        \n",
    "        if (i + 1) % 5 == 0:\n",
    "            print(f'  Chunk {i+1}: {total_raw:,} read, {total_kept:,} kept')\n",
    "\n",
    "except Exception as e:\n",
    "    print(f'Error during chunked read: {e}')\n",
    "\n",
    "print(f'\\nTotal rows read: {total_raw:,}')\n",
    "print(f'Total tech rows kept: {total_kept:,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicates removed: 0\n",
      "Date range: 2023-03-24 07:22:00+00:00 to 2025-05-08 10:16:46+00:00\n",
      "Null dates: 0\n",
      "\n",
      "============================================================\n",
      "Dataset: jobstreet (cleaned, tech only)\n",
      "============================================================\n",
      "Shape: 18,204 rows x 12 columns\n",
      "\n",
      "Column types:\n",
      "  job_id                         int64           nulls:      0 (0.0%)\n",
      "  job_title                      object          nulls:      0 (0.0%)\n",
      "  company                        object          nulls:      0 (0.0%)\n",
      "  descriptions                   object          nulls:      0 (0.0%)\n",
      "  location                       object          nulls:      0 (0.0%)\n",
      "  category                       category        nulls:      0 (0.0%)\n",
      "  subcategory                    category        nulls:      0 (0.0%)\n",
      "  role                           category        nulls:    675 (3.7%)\n",
      "  type                           category        nulls:      0 (0.0%)\n",
      "  salary                         object          nulls:  12507 (68.7%)\n",
      "  listingDate                    datetime64[ns, UTC] nulls:      0 (0.0%)\n",
      "  source                         object          nulls:      0 (0.0%)\n",
      "\n",
      "Exact duplicates: 0\n",
      "Memory usage: 104.0 MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'name': 'jobstreet (cleaned, tech only)',\n",
       " 'rows': 18204,\n",
       " 'cols': 12,\n",
       " 'duplicates': np.int64(0)}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Concatenate all chunks\n",
    "df_jobstreet = pd.concat(cleaned_chunks, ignore_index=True)\n",
    "del cleaned_chunks\n",
    "\n",
    "# Drop exact duplicates on job_id\n",
    "n_before = len(df_jobstreet)\n",
    "df_jobstreet = df_jobstreet.drop_duplicates(subset=['job_id'])\n",
    "print(f'Duplicates removed: {n_before - len(df_jobstreet)}')\n",
    "\n",
    "# Parse listingDate to datetime\n",
    "df_jobstreet['listingDate'] = pd.to_datetime(df_jobstreet['listingDate'], errors='coerce')\n",
    "print(f'Date range: {df_jobstreet[\"listingDate\"].min()} to {df_jobstreet[\"listingDate\"].max()}')\n",
    "print(f'Null dates: {df_jobstreet[\"listingDate\"].isna().sum()}')\n",
    "\n",
    "# Convert category to categorical for memory efficiency\n",
    "for col in ['category', 'subcategory', 'role', 'type']:\n",
    "    df_jobstreet[col] = df_jobstreet[col].astype('category')\n",
    "\n",
    "# Add source\n",
    "df_jobstreet['source'] = 'jobstreet'\n",
    "\n",
    "data_quality_report(df_jobstreet, 'jobstreet (cleaned, tech only)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subcategories (top 20):\n",
      "subcategory\n",
      "Electrical/Electronic Engineering    2348\n",
      "Developers/Programmers               1843\n",
      "Mechanical Engineering               1411\n",
      "Management                           1344\n",
      "Civil/Structural Engineering          969\n",
      "Networks & Systems Administration     936\n",
      "Help Desk & IT Support                934\n",
      "Engineering - Software                837\n",
      "Business/Systems Analysts             747\n",
      "Programme & Project Management        572\n",
      "Process Engineering                   486\n",
      "Environmental Engineering             423\n",
      "Supervisors                           411\n",
      "Security                              355\n",
      "Project Management                    327\n",
      "Engineering Drafting                  326\n",
      "Other                                 301\n",
      "Testing & Quality Assurance           301\n",
      "Automotive Engineering                292\n",
      "Maintenance                           284\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Roles (top 15):\n",
      "role\n",
      "engineer                            526\n",
      "project-engineer                    498\n",
      "software-engineer                   497\n",
      "process-engineer                    296\n",
      "technician                          268\n",
      "information-technology-executive    260\n",
      "electrical-engineer                 253\n",
      "mechanical-engineer                 238\n",
      "software-developer                  218\n",
      "project-manager                     203\n",
      "engineering                         186\n",
      "business-analyst                    175\n",
      "design-engineer                     166\n",
      "services-engineer                   142\n",
      "draftperson_2                       142\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Inspect subcategories and roles within tech\n",
    "print('Subcategories (top 20):')\n",
    "print(df_jobstreet['subcategory'].value_counts().head(20))\n",
    "print(f'\\nRoles (top 15):')\n",
    "print(df_jobstreet['role'].value_counts().head(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: cleaned_jobstreet.parquet (18,204 rows)\n"
     ]
    }
   ],
   "source": [
    "df_jobstreet.to_parquet(PROCESS_DIR / 'cleaned_jobstreet.parquet', index=False)\n",
    "print(f'Saved: cleaned_jobstreet.parquet ({len(df_jobstreet):,} rows)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6 Clean LinkedIn Data (from archive.zip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinkedIn Skill Categories:\n",
      "   skill_abr              skill_name\n",
      "0        ART            Art/Creative\n",
      "1       DSGN                  Design\n",
      "2       ADVR             Advertising\n",
      "3       PRDM      Product Management\n",
      "4       DIST            Distribution\n",
      "5        EDU               Education\n",
      "6       TRNG                Training\n",
      "7       PRJM      Project Management\n",
      "8       CNSL              Consulting\n",
      "9       PRCH              Purchasing\n",
      "10      SUPL            Supply Chain\n",
      "11      ANLS                 Analyst\n",
      "12      HCPR    Health Care Provider\n",
      "13      RSCH                Research\n",
      "14       SCI                 Science\n",
      "15      GENB        General Business\n",
      "16      CUST        Customer Service\n",
      "17      STRA       Strategy/Planning\n",
      "18       FIN                 Finance\n",
      "19      OTHR                   Other\n",
      "20       LGL                   Legal\n",
      "21       ENG             Engineering\n",
      "22        QA       Quality Assurance\n",
      "23        BD    Business Development\n",
      "24        IT  Information Technology\n",
      "25       ADM          Administrative\n",
      "26      PROD              Production\n",
      "27      MRKT               Marketing\n",
      "28        PR        Public Relations\n",
      "29       WRT         Writing/Editing\n",
      "30      ACCT     Accounting/Auditing\n",
      "31        HR         Human Resources\n",
      "32      MNFC           Manufacturing\n",
      "33      SALE                   Sales\n",
      "34      MGMT              Management\n",
      "\n",
      "Total categories: 35\n"
     ]
    }
   ],
   "source": [
    "# LinkedIn Skills Taxonomy (35 categories)\n",
    "df_linkedin_skills = pd.read_csv(LINKEDIN_DIR / 'mappings' / 'skills.csv')\n",
    "print('LinkedIn Skill Categories:')\n",
    "print(df_linkedin_skills)\n",
    "print(f'\\nTotal categories: {len(df_linkedin_skills)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Dataset: linkedin_job_skills (raw)\n",
      "============================================================\n",
      "Shape: 213,768 rows x 2 columns\n",
      "\n",
      "Column types:\n",
      "  job_id                         int64           nulls:      0 (0.0%)\n",
      "  skill_abr                      object          nulls:      0 (0.0%)\n",
      "\n",
      "Exact duplicates: 0\n",
      "Memory usage: 13.9 MB\n",
      "\n",
      "Duplicates removed: 0\n",
      "\n",
      "Skill distribution in LinkedIn jobs:\n",
      "skill_name\n",
      "Information Technology    26137\n",
      "Sales                     22475\n",
      "Management                20861\n",
      "Manufacturing             18185\n",
      "Health Care Provider      17369\n",
      "Business Development      14290\n",
      "Engineering               13009\n",
      "Other                     12608\n",
      "Finance                    8540\n",
      "Marketing                  5525\n",
      "Accounting/Auditing        5461\n",
      "Administrative             4860\n",
      "Customer Service           4292\n",
      "Project Management         3997\n",
      "Analyst                    3858\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# LinkedIn Job-Skills Mapping (213K rows)\n",
    "df_linkedin_job_skills = pd.read_csv(LINKEDIN_DIR / 'jobs' / 'job_skills.csv')\n",
    "data_quality_report(df_linkedin_job_skills, 'linkedin_job_skills (raw)')\n",
    "\n",
    "# Drop duplicates\n",
    "n_before = len(df_linkedin_job_skills)\n",
    "df_linkedin_job_skills = df_linkedin_job_skills.drop_duplicates()\n",
    "print(f'\\nDuplicates removed: {n_before - len(df_linkedin_job_skills)}')\n",
    "\n",
    "# Merge skill names\n",
    "df_linkedin_job_skills = df_linkedin_job_skills.merge(\n",
    "    df_linkedin_skills, on='skill_abr', how='left'\n",
    ")\n",
    "\n",
    "print(f'\\nSkill distribution in LinkedIn jobs:')\n",
    "print(df_linkedin_job_skills['skill_name'].value_counts().head(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: linkedin_skills_taxonomy.parquet (35 rows)\n",
      "Saved: linkedin_job_skills.parquet (213,768 rows)\n"
     ]
    }
   ],
   "source": [
    "# Save LinkedIn data\n",
    "df_linkedin_skills.to_parquet(PROCESS_DIR / 'linkedin_skills_taxonomy.parquet', index=False)\n",
    "df_linkedin_job_skills.to_parquet(PROCESS_DIR / 'linkedin_job_skills.parquet', index=False)\n",
    "print(f'Saved: linkedin_skills_taxonomy.parquet ({len(df_linkedin_skills)} rows)')\n",
    "print(f'Saved: linkedin_job_skills.parquet ({len(df_linkedin_job_skills):,} rows)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.7 Data Quality Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "DATA QUALITY SUMMARY - ALL DATASETS\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dataset</th>\n",
       "      <th>Rows (raw)</th>\n",
       "      <th>Rows (clean)</th>\n",
       "      <th>Columns</th>\n",
       "      <th>Has Skills</th>\n",
       "      <th>Has Dates</th>\n",
       "      <th>Date Range</th>\n",
       "      <th>Has Salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>future_jobs</td>\n",
       "      <td>10000</td>\n",
       "      <td>10000</td>\n",
       "      <td>10</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>2025-01 to 2025-12</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>india_jobs</td>\n",
       "      <td>779</td>\n",
       "      <td>32</td>\n",
       "      <td>14</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>N/A</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>job_market</td>\n",
       "      <td>250</td>\n",
       "      <td>250</td>\n",
       "      <td>11</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>2025-11 (single date)</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>jobstreet</td>\n",
       "      <td>2057213</td>\n",
       "      <td>18204</td>\n",
       "      <td>12</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>2023-03-24 07:22:00+00:00 to 2025-05-08 10:16:...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>linkedin_job_skills</td>\n",
       "      <td>213000</td>\n",
       "      <td>213768</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>N/A</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Dataset  Rows (raw)  Rows (clean)  Columns  Has Skills  \\\n",
       "0          future_jobs       10000         10000       10        True   \n",
       "1           india_jobs         779            32       14        True   \n",
       "2           job_market         250           250       11        True   \n",
       "3            jobstreet     2057213         18204       12       False   \n",
       "4  linkedin_job_skills      213000        213768        3        True   \n",
       "\n",
       "   Has Dates                                         Date Range  Has Salary  \n",
       "0       True                                 2025-01 to 2025-12        True  \n",
       "1      False                                                N/A       False  \n",
       "2       True                              2025-11 (single date)        True  \n",
       "3       True  2023-03-24 07:22:00+00:00 to 2025-05-08 10:16:...        True  \n",
       "4      False                                                N/A       False  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Output files in ml/data/process/:\n",
      "  cleaned_future_jobs.parquet                   0.2 MB\n",
      "  cleaned_india_jobs.parquet                    0.0 MB\n",
      "  cleaned_job_market.parquet                    0.0 MB\n",
      "  cleaned_jobstreet.parquet                     20.9 MB\n",
      "  linkedin_job_skills.parquet                   1.2 MB\n",
      "  linkedin_skills_taxonomy.parquet              0.0 MB\n"
     ]
    }
   ],
   "source": [
    "summary = pd.DataFrame([\n",
    "    {\n",
    "        'Dataset': 'future_jobs',\n",
    "        'Rows (raw)': 10_000,\n",
    "        'Rows (clean)': len(df_future_clean),\n",
    "        'Columns': len(df_future_clean.columns),\n",
    "        'Has Skills': True,\n",
    "        'Has Dates': True,\n",
    "        'Date Range': f\"{df_future_clean['posting_date'].min().strftime('%Y-%m')} to {df_future_clean['posting_date'].max().strftime('%Y-%m')}\",\n",
    "        'Has Salary': True,\n",
    "    },\n",
    "    {\n",
    "        'Dataset': 'india_jobs',\n",
    "        'Rows (raw)': 779,\n",
    "        'Rows (clean)': len(df_india_clean),\n",
    "        'Columns': len(df_india_clean.columns),\n",
    "        'Has Skills': True,\n",
    "        'Has Dates': False,\n",
    "        'Date Range': 'N/A',\n",
    "        'Has Salary': False,\n",
    "    },\n",
    "    {\n",
    "        'Dataset': 'job_market',\n",
    "        'Rows (raw)': 250,\n",
    "        'Rows (clean)': len(df_market_clean),\n",
    "        'Columns': len(df_market_clean.columns),\n",
    "        'Has Skills': True,\n",
    "        'Has Dates': True,\n",
    "        'Date Range': '2025-11 (single date)',\n",
    "        'Has Salary': True,\n",
    "    },\n",
    "    {\n",
    "        'Dataset': 'jobstreet',\n",
    "        'Rows (raw)': 2_057_213,\n",
    "        'Rows (clean)': len(df_jobstreet),\n",
    "        'Columns': len(df_jobstreet.columns),\n",
    "        'Has Skills': False,\n",
    "        'Has Dates': True,\n",
    "        'Date Range': f\"{df_jobstreet['listingDate'].min()} to {df_jobstreet['listingDate'].max()}\",\n",
    "        'Has Salary': True,\n",
    "    },\n",
    "    {\n",
    "        'Dataset': 'linkedin_job_skills',\n",
    "        'Rows (raw)': 213_000,\n",
    "        'Rows (clean)': len(df_linkedin_job_skills),\n",
    "        'Columns': len(df_linkedin_job_skills.columns),\n",
    "        'Has Skills': True,\n",
    "        'Has Dates': False,\n",
    "        'Date Range': 'N/A',\n",
    "        'Has Salary': False,\n",
    "    },\n",
    "])\n",
    "\n",
    "print('\\n' + '=' * 80)\n",
    "print('DATA QUALITY SUMMARY - ALL DATASETS')\n",
    "print('=' * 80)\n",
    "display(summary)\n",
    "\n",
    "# List output files\n",
    "print('\\nOutput files in ml/data/process/:')\n",
    "for f in sorted(PROCESS_DIR.glob('*.parquet')):\n",
    "    size_mb = f.stat().st_size / 1024**2\n",
    "    print(f'  {f.name:45s} {size_mb:.1f} MB')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
